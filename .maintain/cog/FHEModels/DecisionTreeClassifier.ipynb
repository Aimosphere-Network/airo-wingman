{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "Trees are a popular class of algorithm in Machine Learning. In this notebook we build a simple Decision Tree Classifier using `scikit-learn` to show that they can be executed homomorphically using Concrete.\n",
    "\n",
    "Converting a tree working over quantized data to its FHE equivalent takes only a few lines of code thanks to Concrete ML.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use case\n",
    "\n",
    "The use case is a spam classification task from OpenML you can find here: https://www.openml.org/d/44\n",
    "\n",
    "Some pre-extracted features (like some word frequencies) are provided as well as a class - `0` for a normal e-mail and `1` for spam - for 4601 samples.\n",
    "\n",
    "Let's first get the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from concrete.ml.sklearn import DecisionTreeClassifier as ConcreteDecisionTreeClassifier\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, classes = fetch_openml(data_id=44, as_frame=False, cache=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = classes.astype(numpy.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    classes,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = concrete\n",
    "y = normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the sklearn cross-validation tool to find the best hyper parameters for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search: It works by systematically trying out every possible combination of hyperparameters within a specified range or list of values and evaluating the model's performance for each combination using cross-validation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv: sets the number of folds in cross-validation. The training data is split into 10 parts, and the model is trained and validated 10 times, each time using a different part as the validation set and the remaining parts as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_jobs: sets the number of jobs to run in parallel. n_jobs=1 means the tasks will be run sequentially, n_jobs=-1 would use all available processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hyper parameters to tune\n",
    "param_grid = {\n",
    "    \"max_features\": [None, \"auto\", \"sqrt\", \"log2\"],\n",
    "    \"min_samples_leaf\": [1, 10, 100],\n",
    "    \"min_samples_split\": [2, 10, 100],\n",
    "    \"max_depth\": [None, 2, 4, 6, 8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best hyper parameters with cross validation\n",
    "grid_search = GridSearchCV(\n",
    "    ConcreteDecisionTreeClassifier(),\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring=\"average_precision\",\n",
    "    error_score=\"raise\",\n",
    "    n_jobs=1,\n",
    ")\n",
    "gs_results = grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 10, 'min_samples_split': 100}\n",
      "Best score: 0.9302161645088886\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyper parameters:\", gs_results.best_params_)\n",
    "print(\"Best score:\", gs_results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with best hyper parameters\n",
    "model = ConcreteDecisionTreeClassifier(\n",
    "    max_features=gs_results.best_params_[\"max_features\"],\n",
    "    min_samples_leaf=gs_results.best_params_[\"min_samples_leaf\"],\n",
    "    min_samples_split=gs_results.best_params_[\"min_samples_split\"],\n",
    "    max_depth=gs_results.best_params_[\"max_depth\"],\n",
    "    n_bits=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute some metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark model for concrete vs. concrete_fhe vs. normal sklearn\n",
    "model, sklearn_model = model.fit_benchmark(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(min_samples_leaf=10, min_samples_split=100,\n",
      "                       n_bits={'op_inputs': 6, 'op_leaves': 6},\n",
      "                       random_state=22379)\n",
      "DecisionTreeClassifier(min_samples_leaf=10, min_samples_split=100,\n",
      "                       random_state=22379)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(sklearn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average precision on test\n",
    "# pylint: disable=no-member\n",
    "y_pred_concrete = model.predict_proba(x_test)[:, 1]\n",
    "y_pred_sklearn = sklearn_model.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_average_precision = average_precision_score(y_test, y_pred_concrete)\n",
    "sklearn_average_precision = average_precision_score(y_test, y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete average precision score: 0.97\n",
      "Sklearn average precision score: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Concrete average precision score: {concrete_average_precision:0.2f}\")\n",
    "print(f\"Sklearn average precision score: {sklearn_average_precision:0.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Concrete average precision score is not running in FHE here as it would be much longer. If you want to run the model in FHE you can set the argument `fhe` to `execute` in `predict_proba()`. Also, the average precision of the Concrete model may be higher which is likely due to the quantization acting as a kind of regularization which improved the test set metric. However, in general, it should be expected that quantization decreases the average precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 691\n",
      "Number of spams in test samples: 304\n",
      "True Negative (legit mail well classified) rate: 0.9612403100775194\n",
      "False Positive (legit mail classified as spam) rate: 0.03875968992248062\n",
      "False Negative (spam mail classified as legit) rate: 0.14473684210526316\n",
      "True Positive (spam well classified) rate: 0.8552631578947368\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "x_pred = model.predict(x_test)\n",
    "true_negative, false_positive, false_negative, true_positive = confusion_matrix(\n",
    "    y_test, x_pred, normalize=\"true\"\n",
    ").ravel()\n",
    "\n",
    "num_samples = len(y_test)\n",
    "num_spam = sum(y_test)\n",
    "\n",
    "print(f\"Number of test samples: {num_samples}\")\n",
    "print(f\"Number of spams in test samples: {num_spam}\")\n",
    "\n",
    "print(f\"True Negative (legit mail well classified) rate: {true_negative}\")\n",
    "print(f\"False Positive (legit mail classified as spam) rate: {false_positive}\")\n",
    "print(f\"False Negative (spam mail classified as legit) rate: {false_negative}\")\n",
    "print(f\"True Positive (spam well classified) rate: {true_positive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the concrete model\n",
    "circuit = model.compile(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a key for an 8-bit circuit\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a key for an {circuit.graph.maximum_integer_bit_width()}-bit circuit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key generation time: 0.75 seconds\n"
     ]
    }
   ],
   "source": [
    "time_begin = time.time()\n",
    "circuit.client.keygen(force=False)\n",
    "print(f\"Key generation time: {time.time() - time_begin:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the sample size for a faster total execution time\n",
    "FHE_SAMPLES = 10\n",
    "x_test_reduced = x_test[:FHE_SAMPLES]\n",
    "y_pred_reduced = y_pred[:FHE_SAMPLES]\n",
    "y_reference_reduced = y_test[:FHE_SAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.03 seconds per sample\n"
     ]
    }
   ],
   "source": [
    "# Predict in FHE for sample size = 10\n",
    "time_begin = time.time()\n",
    "x_pred_fhe_reduced = model.predict(x_test_reduced, fhe=\"execute\")\n",
    "print(f\"Execution time: {(time.time() - time_begin) / len(x_test):.2f} seconds per sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth:       [0 0 0 1 0 1 0 0 0 0]\n",
      "Prediction sklearn: [0 0 0 1 0 1 0 0 0 0]\n",
      "Prediction FHE:     [0 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Check prediction FHE vs sklearn\n",
    "print(f\"Ground truth:       {y_reference_reduced}\")\n",
    "print(f\"Prediction sklearn: {y_pred_reduced}\")\n",
    "print(f\"Prediction FHE:     {x_pred_fhe_reduced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 predictions are similar between the FHE model and the clear sklearn model.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{numpy.sum(x_pred_fhe_reduced==y_pred_reduced)}/\"\n",
    "    \"10 predictions are similar between the FHE model and the clear sklearn model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict in FHE for actual sample size = 100\n",
    "time_begin = time.time()\n",
    "x_pred_fhe = model.predict(x_test, fhe=\"execute\")\n",
    "print(f\"Execution time: {(time.time() - time_begin) / len(x_test):.2f} seconds per sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prediction FHE vs sklearn\n",
    "print(f\"Ground truth:       {y_reference}\")\n",
    "print(f\"Prediction sklearn: {y_pred}\")\n",
    "print(f\"Prediction FHE:     {x_pred_fhe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{numpy.sum(x_pred_fhe_reduced==y_pred_reduced)}/\"\n",
    "    \"100 predictions are similar between the FHE model and the clear sklearn model.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
